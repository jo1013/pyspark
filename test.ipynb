{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcad221",
   "metadata": {},
   "source": [
    "## Pyspark DB 연결 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    " \n",
    "conf = SparkConf().setAll([('spark.driver.extraClassPath',\n",
    "                            '/usr/share/java/mysql-connector-java-8.0.25.jar')])  # jar path추가.\n",
    " \n",
    "sc = SparkContext(appName=\"TestPySparkJDBC\", conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    " \n",
    "hostname = \"172.23.0.1\"\n",
    "dbname = \"testdb\"\n",
    "jdbcPort = 3306\n",
    "username = \"root\"\n",
    "password = \"root\"\n",
    "jdbc_url = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(hostname,jdbcPort,dbname,username,password)\n",
    " \n",
    "query = '(select 1) AS t'\n",
    "df1 = sqlContext.read.format('jdbc').options(driver='com.mysql.jdbc.Driver', url=jdbc_url, dbtable=query).load()\n",
    "df1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbconfig.py\n",
    "class MysqlController:\n",
    "    def __init__(self, hostname, username, password, testdb):\n",
    "        self.conn = pymysql.connect(host=hostname, user= username, password=password, db=testdb,charset='utf8')\n",
    "        self.curs = self.conn.cursor()\n",
    "\n",
    "    def insert_total(self,total):\n",
    "        sql = 'INSERT INTO entire_nodes (count_of_nodes) VALUES (%s)'\n",
    "        self.curs.execute(sql,(total,))\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '12_19_stt.csv'\n",
    "sdfData = scSpark.read.csv(data_file, header=True, sep=\",\", encoding='euc-kr').cache()\n",
    "print('Total Records = {}'.format(sdfData.count()))\n",
    "sdfData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8da052",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sdfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914beba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars\", \"/usr/share/java/mysql-connector-java-8.0.22.jar\") \\\n",
    "    .master(\"local\").appName(\"PySpark_MySQL_test\").getOrCreate()\n",
    "\n",
    "wine_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/testDB\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"testtest\") \\\n",
    "    .option(\"user\", \"root\").option(\"password\", \"root\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "cnx = mysql.connector.connect(user='root', password='root',\n",
    "                              host='127.0.0.1',\n",
    "                              database='testdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f603791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.add_packages('mysql:mysql-connector-java:8.0.11')\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('test') \\\n",
    "        .master('local[*]') \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"<path to mysql-connector-java-5.1.49-bin.jar>\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:mysql://localhost/testdb\").option(\"driver\",\"com.mysql.jdbc.Driver\").option(\"dbtable\",\"testtest\").option(\"user\",\"root\").option(\"password\",\"root\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc5867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
